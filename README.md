# iLumina â€“ Privacyâ€‘Preserving Exam Reader (w/ AnythingLLM)

**Desktop app for accessible, offline exam reading.**  
iLumina uses a **webcam/PDF âœ onâ€‘device processing âœ offline TTS** pipeline so exam content never leaves the machine. It adds **voiceâ€‘only navigation** and an **AnythingLLM â€œreaderâ€‘onlyâ€ integration** that identifies questions but never provides answers.

> **Why:** Support students with dyslexia or visual impairments while respecting strict exam rules (no internet, privacy by default, lowâ€‘latency, inclusive UX).

---
## ğŸ‘¥ Authors & Maintainers

| Name | GitHub | Affiliation |
|---|---|---|
| Rushali | [@<handle>](https://github.com/rushalimoteria8) | NYU |
| Rujuta | [@<handle>](https://github.com/Rajoshi11) | NYU |
| Mahima |  [@<handle>](https://github.com/mahi397) | NYU |
| Gopala Krishna (GK) | [@<handle>](https://github.com/igopalakrishna) | NYU |
| Sarang | [@<handle>](https://github.com/SARANG1018) | NYU |


**Project Lead:** Gopala Krishna Abba  
**Maintainers:** Teammate 1, Teammate 2  
**Acknowledgments:** Mentors / sponsors, etc.
--

## Highlights

- **Handsâ€‘free exam mode:** Voice commands like â€œrepeatâ€, â€œrepeat slowerâ€, â€œready to answerâ€, â€œnext/previous questionâ€.
- **Offline Textâ€‘toâ€‘Speech:** `pyttsx3` + `pygame` with voice & rate controls.
- **PDF ingestion:** PyMuPDF text extraction; optional OCR (EasyOCR) for scans.
- **Question detection via AnythingLLM:** Extracts questions/options from the PDF text and reads them aloud; **explicitly avoids giving hints/answers**.
- **Whisperâ€‘based speechâ€‘toâ€‘text:** Live voice input with ONNX/Torch â€œstandaloneâ€ Whisper; attempts **QNN (SnapdragonÂ® NPU) âœ CPU fallback**.
- **Clean API + Electron UI:** Flask backend at `127.0.0.1:5000` with an Electron frontâ€‘end.
- **Privacyâ€‘first:** Entire flow runs locally; no exam data is uploaded.

---

## Repository Layout

```text
iLumina-anythingllm/
â”œâ”€backend/
â”‚  â”œâ”€src/
â”‚  â”‚  â””â”€workspaces.py
â”‚  â”œâ”€agentic_exam_workflow.py
â”‚  â”œâ”€anythingllm_config.yaml
â”‚  â”œâ”€anythingllm_integration.py
â”‚  â”œâ”€app.py
â”‚  â”œâ”€pdf_processor.py
â”‚  â”œâ”€requirements.txt
â”‚  â”œâ”€simple_pdf_processor.py
â”‚  â”œâ”€simple_whisper_processor.py
â”‚  â”œâ”€standalone_whisper_integration.py
â”‚  â”œâ”€test_integration.py
â”‚  â”œâ”€tts_engine.py
â”‚  â””â”€whisper_voice_controller.py
â”œâ”€frontend/
â”‚  â”œâ”€renderer/
â”‚  â”‚  â”œâ”€app.js
â”‚  â”‚  â”œâ”€index.html
â”‚  â”‚  â”œâ”€pipeline.html
â”‚  â”‚  â”œâ”€styles.css
â”‚  â”‚  â””â”€test.html
â”‚  â”œâ”€main.js
â”‚  â”œâ”€package-lock.json
â”‚  â””â”€package.json
â”œâ”€reference/
â”‚  â””â”€WhisperApp.py
â”œâ”€src/
â”‚  â”œâ”€LiveTranscriber.py
â”‚  â”œâ”€LiveTranscriber_standalone.py
â”‚  â”œâ”€model.py
â”‚  â”œâ”€pdf_processor.py
â”‚  â”œâ”€standalone_model.py
â”‚  â”œâ”€standalone_whisper.py
â”‚  â”œâ”€test_mic.py
â”‚  â”œâ”€TestApplication.py
â”‚  â””â”€tts_engine.py
â”œâ”€uploads/
â”‚  â”œâ”€68cc83cf-c60c-4836-9d1e-adce7e1e910c_Undergrad_English_Sample_5MCQ.pdf
â”‚  â””â”€dc781842-3d1a-470d-8a1e-488f20208dd0_Undergrad_English_Sample_5MCQ.pdf
â”œâ”€.gitignore
â”œâ”€build-requirements.txt
â”œâ”€build.bat
â”œâ”€build.ps1
â”œâ”€BUILD_EXECUTABLE.md
â”œâ”€build_executable.py
â”œâ”€build_test_app.py
â”œâ”€CODE_OF_CONDUCT.md
â”œâ”€comprehensive_test.py
â”œâ”€CONTRIBUTING.md
â”œâ”€debug_anythingllm.py
â”œâ”€diagnose_executable.bat
â”œâ”€extract_mel_filters.py
â”œâ”€FINAL_REPORT.py
â”œâ”€LICENSE
â”œâ”€mel_filters.npz
â”œâ”€package-lock.json
â”œâ”€package.json
â”œâ”€README.md
â”œâ”€README_TEST_APP.md
â”œâ”€requirements.txt
â”œâ”€simple_test.py
â”œâ”€SUMMARY.md
â”œâ”€test-app-requirements.txt
â”œâ”€test_anythingllm_integration.py
â”œâ”€test_anythingllm_methods.py
â”œâ”€test_exam.py
â”œâ”€test_exam.txt
â”œâ”€test_functionality.py
â”œâ”€test_voice_improvements.py
â””â”€WhisperTranscriber.spec
```

Key bits:
- **backend/**: Flask API, PDF processing, TTS, Whisper control, AnythingLLM client.
- **src/**: Standalone Whisper implementation (+ ONNX session fallback logic), test app, TTS.
- **frontend/**: Electron desktop shell and renderer (UI pages).
- **uploads/**: Sample PDFs for quick testing.
- **BUILD_EXECUTABLE.md**: PyInstaller packaging notes.

---

## Quickstart

### 1) System prerequisites
- **Python** 3.10+ (recommended)
- **Node.js** 18+ and **npm**
- **FFmpeg** (recommended for robust audio on some systems)
- **PortAudio** (macOS/Linux only) if you run microphone capture locally  
  - macOS: `brew install portaudio` then `pip install pyaudio`
- **Windows on Snapdragon (optional):** QualcommÂ® QNN provider for ONNX Runtime (see _Acceleration_ below)

### 2) Clone & Python environment

```bash
git clone <your-repo-url>.git
cd iLumina-anythingllm

# create and activate a venv
python -m venv .venv
# Windows
.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate

# core deps (requirements.txt is UTFâ€‘16 encoded)
python - <<'PY'
from pathlib import Path
txt = Path('requirements.txt').read_text(encoding='utf-16')
Path('requirements-utf8.txt').write_text(txt, encoding='utf-8')
print('Wrote requirements-utf8.txt')
PY
pip install -r requirements-utf8.txt

# backend extras (Flask API)
pip install -r backend/requirements.txt

# optional OCR & enhanced test app features
pip install -r test-app-requirements.txt
```

> If `onnxruntime-qnn` fails on pip (itâ€™s not published on PyPI), you can **comment it out** and run on CPU. See **Acceleration** below for QNN install options.

### 3) Frontend (Electron)

```bash
cd frontend
npm install
npm run dev   # or: npm start
```

### 4) Backend (Flask API)

In another terminal:

```bash
cd backend
python app.py
```

Health check:

```bash
curl http://127.0.0.1:5000/api/health
# => {"status":"healthy","whisper":"available|unavailable","tts":"available","pdf":"available"}
```

---

## Configuration

### AnythingLLM (readerâ€‘only)
Update `backend/anythingllm_config.yaml`:

```yaml
api_key: "<YOUR-ANYTHINGLLM-API-KEY>"
model_server_base_url: "http://localhost:3001/api"
workspace_slug: "ilumina"
stream: true
max_tokens: 500
temperature: 0.7

# Strictly â€œreaderâ€ persona
system_prompt: "You are an AI exam reading assistant... do not provide hints or answers."
exam_reader_prompt: "Read questions exactly as written. Slow, clear, numbered."
navigation_prompt: "Assist only with navigation (repeat, slower, next/prev)."
boundary_reminder: "I can only read questions and options; I cannot help solve them."
```

> **Security tip:** never commit real API keys. Use a private YAML or envâ€‘subst during deploy.

### Accessibility defaults
The same YAML exposes **reading pace**, **option pauses**, and **question numbering** flagsâ€”tune them to your room policy.

---

## How It Works

1. **Upload PDF** (`/api/exam/upload`) â†’ text extracted via PyMuPDF (OCR optional).
2. **AnythingLLM pass**: splits text into questions/options with a â€œreaderâ€‘onlyâ€ system prompt.
3. **Kick off exam** (`/api/agentic/start-exam`): TTS instructions + readiness check.
4. **Voice control** (`/api/agentic/voice-command`): Whisper transcribes your mic input.
5. **Answer capture**: When you say **â€œready to answerâ€**, your next utterance is stored for the current question.
6. **Finish** (`/api/agentic/finish-exam`): Export the answer sheet JSON.

### Voice Commands (examples)
- `repeat` â€” reâ€‘read the current question
- `repeat slower` â€” reâ€‘read more slowly
- `next question` / `previous question`
- `ready to answer` â€” the next utterance is recorded as your answer
- `finish exam` â€” end session and return answer sheet

---

## REST API (selected)

Base URL: `http://127.0.0.1:5000`

| Method | Path | Purpose |
|---|---|---|
| GET | `/api/health` | Service status (Whisper/TTS/PDF) |
| POST | `/api/exam/upload` | Formâ€‘data PDF upload (`file`) |
| POST | `/api/agentic/start-exam` | Start agentic flow; returns first audio + status |
| POST | `/api/agentic/voice-command` | `{ "transcribed_text": "repeat slower" }` |
| POST | `/api/agentic/finish-exam` | Finish and return final answer sheet |
| GET | `/api/audio/voices` | List available TTS voices |
| GET | `/api/audio/file/<filename>` | Stream generated `.wav` |

Minimal cURL to test upload:

```bash
curl -F "file=@uploads/dc781842-3d1a-470d-8a1e-488f20208dd0_Undergrad_English_Sample_5MCQ.pdf" \
  http://127.0.0.1:5000/api/exam/upload
```

---

## Local Testing

Quick scripts in the repo:
- `test_exam.py` â€“ creates a tiny PDF on the fly and drives the core endpoints
- `test_anythingllm_integration.py` â€“ ping the Aâ€‘LLM workspace (requires config)
- `comprehensive_test.py` â€“ endâ€‘toâ€‘end checks
- `src/TestApplication.py` â€“ simple UI runner for the standalone pipeline
- `src/test_mic.py` â€“ verify microphone capture

Example:

```bash
python test_exam.py
```

---

## Acceleration (QNN on SnapdragonÂ®)

The â€œstandaloneâ€ Whisper can run with **ONNX Runtime**. The code will try loading the **QNN Execution Provider** first and gracefully fall back to CPU if unavailable:

- `src/standalone_model.py` â†’ `get_onnx_session_with_fallback(...)`

If youâ€™re on a **Copilot+ PC (Snapdragon X series)** and have access to Qualcommâ€™s ONNX Runtime QNN build, install it per vendor docs. If not, remove/comment `onnxruntime-qnn` in `requirements.txt` and run CPU.

> If you saw `ERROR: No matching distribution found for onnxruntime-qnn`, thatâ€™s expected on PyPI; use vendor wheels or keep CPU.

---

## Building an .exe / desktop bundle

### PyInstaller (Windows/macOS/Linux)

```bash
pip install -r build-requirements.txt
python build_executable.py
# or: pyinstaller WhisperTranscriber.spec
```

Artifacts land under `dist/`. See **BUILD_EXECUTABLE.md** for details.

### Electron app

```bash
cd frontend
npm run build     # use electronâ€‘builder targets (nsis/dmg/AppImage)
```

---

## Troubleshooting

- **PyMuPDF not available:** install `PyMuPDF` (see `test-app-requirements.txt`). Without it, PDF extraction is limited.
- **PyAudio install errors (macOS):** `brew install portaudio && pip install pyaudio`.
- **QNN package not found:** comment `onnxruntime-qnn` or install the vendor wheel; CPU fallback will work.
- **Mic permissions (macOS/Windows):** grant microphone access to Python/Electron.
- **TTS not speaking:** on headless servers, `pyttsx3` may need an audio device; prefer running on a desktop OS.

---

## License

MIT â€” see [LICENSE](LICENSE).

---

##Contributing

Please read:
- [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md)
- [CONTRIBUTING.md](CONTRIBUTING.md)

PRs that improve accessibility, add languages, or harden the offline story are especially welcome.

---

## Acknowledgments

- **Qualcomm Whisper and EverythingLLM ** (speechâ€‘toâ€‘text)
- **Qualcomm AI assets** and QNN EP notes for NPU acceleration
- **PyMuPDF**, **EasyOCR**, **pyttsx3**, **pygame**, and the **Electron** community
